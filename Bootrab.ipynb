import numpy as np
from scipy import stats
from matplotlib import pyplot as plt
dem_share_PA = [60.08, 40.64, 36.07, 41.21, 31.04, 43.78, 44.08, 46.85, 44.71, 46.15, 63.10, 52.20, 43.18, 40.24, 39.92, 47.87, 37.77, 40.11, 49.85, 48.61, 38.62, 54.25, 34.84, 47.75, 43.82, 55.97, 58.23, 42.97, 42.38, 36.11, 37.53, 42.65, 50.96, 47.43, 56.24, 45.60, 46.39, 35.22, 48.56, 32.97, 57.88, 36.05, 37.72, 50.36, 32.12, 41.55, 54.66, 57.81, 54.58, 32.88, 54.37, 40.45, 47.61, 60.49, 43.11, 27.32, 44.03, 33.56, 37.26, 54.64, 43.12, 25.34, 49.79, 83.56, 40.09, 60.81, 49.81]
dem_share_OH = [56.94, 50.46, 65.99, 45.88, 42.23, 45.26, 57.01, 53.61, 59.10, 61.48, 43.43, 44.69, 54.59, 48.36, 45.89, 48.62, 43.92, 38.23, 28.79, 63.57, 38.07, 40.18, 43.05, 41.56, 42.49, 36.06, 52.76, 46.07, 39.43, 39.26, 47.47, 27.92, 38.01, 45.45, 29.07, 28.94, 51.28, 50.10, 39.84, 36.43, 35.71, 31.47, 47.01, 40.10, 48.76, 31.56, 39.86, 45.31, 35.47, 51.38, 46.33, 48.73, 41.77, 41.32, 48.46, 53.14, 34.01, 54.74, 40.67, 38.96, 46.29, 38.25, 6.80, 31.75, 46.33, 44.90, 33.57, 38.10, 39.67, 40.47, 49.44, 37.62, 36.71, 46.73, 42.20, 53.16, 52.40, 58.36, 68.02, 38.53, 34.58, 69.64, 60.50, 53.53, 36.54, 49.58, 41.97, 38.11]
print("Mean:\t\t", round(np.mean(dem_share_PA),2))
print("Median:\t\t", round(np.median(dem_share_PA),2))
print("SD:\t\t", round(np.std(dem_share_PA),2))
print("Sample Size:\t", len(dem_share_PA))
plt.hist(dem_share_PA, bins=30, alpha=0.25)
plt.grid()
plt.show()
print("Mean:\t\t", round(np.mean(dem_share_OH),2))
print("Median:\t\t", round(np.median(dem_share_OH),2))
print("SD:\t\t", round(np.std(dem_share_OH),2))
print("Sample Size:\t", len(dem_share_OH))
plt.hist(dem_share_OH, bins=30, alpha=0.25)
plt.grid()
plt.show()
# Lower Critical Value
lcv = stats.norm.ppf(0.05)
print("Lower CV:", round(lcv,3))
# Upper Critical Value
ucv = stats.norm.ppf(1 - 0.05)
print("Upper CV: ", round(ucv,3))
sqrt(std(sample_1)^2 / len(sample_1) + std(sample_2)^2 / len(sample_2))
se = np.sqrt( np.std(dem_share_PA)**2 / len(dem_share_PA) + np.std(dem_share_OH)**2 / len(dem_share_OH) )
print(round(se,3))
emp_diff_means = np.mean(dem_share_PA) - np.mean(dem_share_OH)
print(round(emp_diff_means,3))
lower_lim_an = emp_diff_means + lcv * se
upper_lim_an = emp_diff_means + ucv * se
print(round(lower_lim_an,3), 'to', round(upper_lim_an,3))
def boot_diff(sample_1, sample_2, base_stat_func, num_iter):
    
    # initialize the list for holding the bootstrap replicates
    bs_replicates = []
    # iterate for the specified number of bootstrapping iterations
    for i in range(num_iter):
        # create the resampled sets of data of the original lengths (WITH REPLACEMENT)
        sample_1_bs = np.random.choice(sample_1, len(sample_1))
        sample_2_bs = np.random.choice(sample_2, len(sample_2))
        # compute the BS replicate using the specified base statistical function and append it to the list of BS replicates
        bs_replicates += [base_stat_func(sample_1_bs) - base_stat_func(sample_2_bs)]
        
    return bs_replicate
bs_replicates = boot_diff(dem_share_PA, dem_share_OH, np.mean, 100000)
plt.hist(bs_replicates, bins=50, alpha = 0.25)
plt.grid()
plt.show()
print("Mean:  ", round(np.mean(bs_replicates),3))
print("Median:", round(np.median(bs_replicates),3))
# the lower limit will be the 5th percentile
lower_lim_bs = np.percentile(bs_replicates,5)
# the upper limit will be the 95th percentile (5's complement to 100)
upper_lim_bs = np.percentile(bs_replicates,95)
print(round(lower_lim_bs,3), 'to', round(upper_lim_bs,3))
BS_stat_mean +/- t * SE
Zc = [(sample_mean_1 - sample_mean_2) - (hyp_difference)] / SE
Zc = (emp_diff_means - 0) / 1.585
print(round(Zc,3))
bs_replicates = boot_diff(dem_share_PA, dem_share_OH, np.mean, 100000)
bs_replicates_shifted = bs_replicates - np.mean(bs_replicates) - 0
print("Replicate 25th percentile, mean, 75th percentile:",
      round(np.percentile(bs_replicates_shifted, 25),3), 
      round(np.mean(bs_replicates_shifted)+1e-4, 2), 
      round(np.percentile(bs_replicates_shifted, 75),3))
print("Replicate Std Dev:", round(np.std(bs_replicates_shifted),3)) # the analytical version is 1.585
emp_diff_pctile_rnk = stats.percentileofscore(bs_replicates_shifted, emp_diff_means)
print("Empirical difference of the sample means:", round(emp_diff_means, 3))
print("Its percentile rank among the replicates:", round(emp_diff_pctile_rnk, 2), "% (AUC to the left)")
auc_left = round(emp_diff_pctile_rnk / 100, 4)
auc_rght = round(1 - emp_diff_pctile_rnk / 100, 4)
print("AUC to the left:  ", auc_left)
print("AUC to the right: ", auc_rght)
auc_tail = auc_left if auc_left < auc_rght else auc_rght
p_val = round(auc_tail * 2, 4)
print("P-value:", p_val)
Zc = (emp_diff_means - (-2)) / 1.585
print(round(Zc,4))
round(stats.norm.ppf(0.9),4)
# use scipy.stats.norm.cdf(), which is the inverse of stats.norm.ppf()
auc_left = round(stats.norm.cdf(1.9926), 4)
print("AUC to the left:", auc_left)
p_val = 1 - auc_left
print("P-value:", p_val)
bs_replicates = boot_diff(dem_share_PA, dem_share_OH, np.mean, 100000)
bs_replicates_shifted = bs_replicates - np.mean(bs_replicates) - 2
print(“Replicate 25th percentile, mean, 75th percentile:”,
 round(np.percentile(bs_replicates_shifted, 25),3), “~”,
 round(np.mean(bs_replicates_shifted)+1e-4, 2), “~”,
 round(np.percentile(bs_replicates_shifted, 75),3))
print(“Replicate Std Dev:”, round(np.std(bs_replicates_shifted),3)) 
# the analytical version is 1.585
emp_diff_pctile_rnk = stats.percentileofscore(bs_replicates_shifted, emp_diff_means)
print("Empirical difference of the sample means:", round(emp_diff_means, 3))
print("Its percentile rank among the replicates:", round(emp_diff_pctile_rnk, 2), "% (AUC to the left)")
auc_left = round(emp_diff_pctile_rnk / 100, 4)
auc_rght = round(1 - emp_diff_pctile_rnk / 100, 4)
print("AUC to the left:  ", auc_left)
print("AUC to the right: ", auc_rght)
def perm_diff(sample_1, sample_2, base_stat_func, num_iter):
    
    # initialize the list for the test statistic replicate
    perm_replicates = []
    # iterate for the specified number of iterations
    for i in range(num_iter):
        # concatenate the two samples into one
        samples_app = sample_1 + sample_2
        # permute the entire appended set (making this complete combined resampling WITHOUT REPLACEMENT)
        samples_perm = np.random.permutation(samples_app)
        # create the hypothesized samples by:
        #  pretending that the first len(sample_1) elements is the first sample
        sample_1_hyp = samples_perm[:len(sample_1)]
        #  and the rest is the second sample
        sample_2_hyp = samples_perm[ len(sample_1):]
        # compute the test statistic replicate and append it to the list of permutation replicates
        perm_replicates += [base_stat_func(sample_1_hyp) - base_stat_func(sample_2_hyp)]
return perm_replicates
perm_replicates = perm_diff(dem_share_PA, dem_share_OH, np.mean, int(1e5))
plt.hist(perm_replicates, bins=50, alpha=0.25)
plt.grid()
plt.show()
# looking at the absolute values on both tails and comparing them to the absolute empirical value simultaneously
p_val_2t_a = sum(np.abs(perm_replicates) >= abs(emp_diff_means)) / len(perm_replicates)
print(round(p_val_2t_a, 4))
# Get the percentile rank of the NEGATIVE ABSOLUTE empirical value and take it as-is (looking at the left tail)
lt_p_val = stats.percentileofscore(perm_replicates, -abs(emp_diff_means) ) / 100
# Get the percentile rank of the ABSOLUTE empirical value and take the complement of it (looking at the right tail)
rt_p_val = 1 - stats.percentileofscore(perm_replicates, abs(emp_diff_means) ) / 100
# Add the two half-values together
p_val_2t_b = lt_p_val + rt_p_val
print(round(p_val_2t_b, 4))
# draw the raw random data for the control sample from a normal distribution
np.random.seed(5)
cs_raw = np.random.normal(loc=75.0, scale=9.0, size=100)
# "pivot" the right tail around 100 with a multiplier 
# (there should not be many cases)
cs = [score - (score - 100)*1.5 if score > 100 else score for score in cs_raw]
# draw the raw random data for the treatment sample from a normal distribution 
# with a slightly larger mean and a significantly larger standard deviation than for the control sample
# (it is expected that there will be quite a few instances here that go over 100)
ts_raw = np.random.normal(loc=78.0, scale=13, size=100)
# "pivot" the right tail around 100 with a multiplier, which caps the score at the max,
# making the right tail much thicker and shorter
ts = [score - (score - 100)*2.0 if score > 100 else score for score in ts_raw]
fig = plt.gcf()
fig.set_size_inches(16,8)
plt.hist(cs, bins=20, alpha=0.15, label = 'control group')
plt.hist(ts, bins=20, alpha=0.15, color='r', label = 'treatment group')
plt.legend()
plt.grid()
plt.show()
print("Control   mean, 10th, 25th, 50th, 75th, 90th pctiles:", 
      round(np.mean(cs),2), "~", 
      round(np.percentile(cs,10),2), "~",
      round(np.percentile(cs,25),2), "~",
      round(np.percentile(cs,50),2), "~",
      round(np.percentile(cs,75),2), "~",
      round(np.percentile(cs,90),2))
print("Treatment mean, 10th, 25th, 50th, 75th, 90th pctiles:", 
      round(np.mean(ts),2), "~", 
      round(np.percentile(ts,10),2), "~",
      round(np.percentile(ts,25),2), "~",
      round(np.percentile(ts,50),2), "~",
      round(np.percentile(ts,75),2), "~",
      round(np.percentile(ts,90),2))
boot_diff_mean_scores = boot_diff(ts, cs, np.mean, int(1e5))
# DON'T FORGET TO SHIFT TO THE HYPOTHESIZED MEAN (0 in this case)
boot_diff_mean_scores = boot_diff_mean_scores - np.mean(boot_diff_mean_scores) + 0
perm_diff_mean_scores = perm_diff(ts, cs, np.mean, int(1e5))
fig = plt.gcf()
fig.set_size_inches(10,5)
plt.hist(boot_diff_mean_scores, bins=50, alpha=0.15, label = 'boot repl')
plt.hist(perm_diff_mean_scores, bins=50, alpha=0.15, color='r', label = 'perm repl')
plt.legend()
plt.grid()
plt.show()
emp_val = np.mean(ts) - np.mean(cs)
boot_p_val = 1 - stats.percentileofscore(boot_diff_mean_scores, emp_val)/100
print("p-value by bootstrapping:\t", round(boot_p_val,5))
perm_p_val = 1 - stats.percentileofscore(perm_diff_mean_scores, emp_val)/100
print("p-value by permutation:\t\t", round(perm_p_val,5))
def low_quart_mean(arr):
    first_percentile = np.percentile(arr, 25)
    filt_arr = [x for x in arr if x <= first_percentile]
    return np.mean(filt_arr)
low_quart_mean(ts)
low_quart_mean(cs)
boot_diff_lqms = boot_diff(ts, cs, low_quart_mean, int(1e5))
# DON'T FORGET TO SHIFT TO THE HYPOTHESIZED MEAN (0 in this case)
boot_diff_lqms = boot_diff_lqms - np.mean(boot_diff_lqms) + 0
perm_diff_lqms = perm_diff(ts, cs, low_quart_mean, int(1e5))
fig = plt.gcf()
fig.set_size_inches(10,5)
plt.hist(boot_diff_lqms, bins=50, alpha=0.15, label = 'boot repl')
plt.hist(perm_diff_lqms, bins=50, alpha=0.15, color='r', label = 'perm repl')
plt.legend()
plt.grid()
plt.show()
emp_val = low_quart_mean(ts) - low_quart_mean(cs)
boot_p_val = stats.percentileofscore(boot_diff_lqms, emp_val)/100
print("p-value by bootstrapping:\t", round(boot_p_val,5))
perm_p_val = stats.percentileofscore(perm_diff_lqms, emp_val)/100
print("p-value by permutation:\t\t", round(perm_p_val,5))





























